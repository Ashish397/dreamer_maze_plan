defaults:
  seed: 996
  steps: 1e6
  device: 'cuda:0'
  precision: 16
  log_every: 128

  # Environment
  envs: 1
  num_actions: 3
  reward_EMA: True
  time_limit: 4096
  action_repeat: 1

  # Model
  dyn_hidden: 512
  dyn_deter: 512
  dyn_stoch: 32
  dyn_discrete: 32
  dyn_rec_depth: 2
  dyn_mean_act: 'none'
  dyn_std_act: 'sigmoid2'
  dyn_min_std: 0.1
  grad_heads: ['decoder', 'reward', 'cont', 'entropy']
  units: 512
  act: 'SiLU'
  norm: True
  m_encoder:
    {mlp_keys: '$^', cnn_keys: 'map', act: 'SiLU', norm: True, cnn_depth: 64, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, symlog_inputs: True}
  v_encoder:
    {mlp_keys: '$^', cnn_keys: 'image', act: 'SiLU', norm: True, cnn_depth: 64, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, symlog_inputs: True}
  decoder:
    {mlp_keys: '$^', cnn_keys: ['image', 'map'], act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, cnn_sigmoid: False, image_dist: mae, vector_dist: symlog_mse, outscale: 1.0}
  actor:
    {layers: 2, dist: 'normal', entropy: 3e-4, unimix_ratio: 0.01, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  Q:
    {layers: 2, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 1.0} #CHANGE
  critic:
    {layers: 2, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 1.0} #CHANGE
  reward_head:
    {layers: 2, dist: 'symlog_disc', loss_scale: 1.0, outscale: 1.0} #CHANGE
  entropy_head:
    {layers: 2, dist: 'symlog_disc', loss_scale: 1.0, outscale: 1.0} #CHANGE
  cont_head:
    {layers: 2, loss_scale: 1.0, outscale: 1.0}
  dyn_scale: 0.5
  rep_scale: 0.1
  kl_free: 1.0
  weight_decay: 0.0
  unimix_ratio: 0.01
  initial: 'learned'

  # Training
  batch_size: 16 #B
  batch_length: 64 #64
  train_ratio: 2
  train_steps: 1
  model_lr: 1e-4
  opt_eps: 1e-8
  grad_clip: 1000
  dataset_size: 1048576
  opt: 'adam'
  random_sample_steps: 0 #IMPLEMENT

  # Behavior.
  discount: 0.997
  discount_lambda: 0.95
  imag_horizon: 15
  imag_gradient: 'dynamics'
  imag_gradient_mix: 0.0
  eval_state_mean: False

  # Exploration
  expl_behavior: 'greedy'
  expl_until: 10000

  # experiment_folders:
  exp_name: 'ppo_tests'
  exp_date: 'ppo_ray_02_poro_reco_param_double_head_real_seed_996'
  path_root: 'C:\\Users\\as1748\\projects\\dreamer_w_ant_cur\\dream3_uq_ent_head_single_disc'

  env_name: 'MiniWorld-MazeCA-v0'
  
  # ppo_behavior
  autoencoder_loss_lambda: 1

  # plan_behavior:
  plan_max_horizon: 15
  plan_choices: 256
  train_every: 32
  sub_batch_size: 64
  num_epochs: 30
  buffer_size: 32768
  clip_epsilon: 0.2
  gamma: 0.99
  lmbda: 0.95
  entropy_eps: 0.1
  num_cells: 256
  lr: 0.003
  seq_length: 32
  buffer_minimum: 128
  meta_action_quant: 5                     # used in CategoricalSpec
  num_meta_action_lwr: 2                     # used in CategoricalSpec
  meta_policy_input_dim_hgr: 11272          # input to meta actor MLP - plan prob (e.g., 8192+1536+1536+1+3+1+1+2)
  meta_policy_input_dim_lwr: 9734          # input to meta actor MLP - plan param (e.g., 8192+1536+1536+1+3+1+1+2)
  random_plan_until: 

  maze_config:
    obs_height: 64
    obs_width: 64
    domain_rand: False
    obs_channels: 3
    include_maps: True
    fluff: 2
    decay_param: 1.0
    porosity: 0.2


ppo_config:
  # framework: "torch"
  ppo_steps: 1398576
  num_workers: 0
  num_gpus: 1
  num_envs_per_worker: 1

  # Rollouts
  rollout_fragment_length: 512
  train_batch_size: 16384        # can push higher because you have the GPU for it
  sgd_minibatch_size: 2048       # big minibatches
  num_sgd_iter: 10
  batch_mode: "complete_episodes"

  # PPO specific
  lr: 3e-5                       # even a little lower lr is safer
  gamma: 0.995
  clip_param: 0.1
  entropy_coeff: 0.001
  vf_loss_coeff: 1.0
  kl_target: 0.01

  # Optimization
  grad_clip: 0.5

  # policy: ray_ppo.RayAEPPOTorchPolicy

  model:
    custom_model: split_vision_model
    vf_share_layers: False
    conv_filters: [
      [32, [5,5], 3],
      [64, [5,5], 3],
      [128, [4,4], 2],
      [256, [4,4], 1],
    ]
    fcnet_hiddens: [512]
    fcnet_activation: relu
    use_lstm: False
    framestack: True     # Ray will stack frames automatically, helps navigation (optional)
    grayscale: False     # Your obs is RGB already

